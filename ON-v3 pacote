import os
import math
import random
import argparse
from typing import List, Iterator, Optional, Tuple, Dict

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers
from datasets import load_dataset

from PIL import Image
from torchvision import transforms
import torchaudio

try:
    from playwright.sync_api import sync_playwright, TimeoutError as PWTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except Exception:
    PLAYWRIGHT_AVAILABLE = False

DEFAULT_VOCAB_SIZE = 2_000_000
DEFAULT_MAX_SEQ = 2048
DEFAULT_EMBED = 1024
DEFAULT_LAYERS = 12
DEFAULT_HEADS = 16
DEFAULT_BATCH = 2
CHECKPOINT_DIR = "checkpoints_on"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATASET_IDS_TEXT = [
    "wikipedia",
    "openwebtext",
    "bookcorpus",
    "pile",
    "cc_news",
    "wikitext",
    "arxiv",
    "pubmed",
    "europarl",
    "open_subtitles",
]

DATASET_IDS_IMAGE = [
    "coco",
    "openimages",
    "image_corpus_placeholder"
]

DATASET_IDS_AUDIO = [
    "mozilla-foundation/common_voice",
    "voxpopuli",
    "speech_commands"
]

class MiniNavigator:
    def __init__(self, headless: bool = True, timeout:int = 15000):
        self.headless = headless
        self.timeout = timeout
        self.playwright = None
        self.browser = None
        self.page = None
        if not PLAYWRIGHT_AVAILABLE:
            print("[MiniNavigator] Playwright não instalado. Instale com `pip install playwright` e rode `playwright install`.")
            return
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=self.headless)
        self.context = self.browser.new_context()
        self.page = None

    def new_page(self):
        if not PLAYWRIGHT_AVAILABLE:
            return None
        if self.page:
            try:
                self.page.close()
            except Exception:
                pass
        self.page = self.context.new_page()
        self.page.set_default_timeout(self.timeout)
        return self.page

    def search(self, query: str, engine: str = "bing"):
        if not PLAYWRIGHT_AVAILABLE:
            return {"error": "Playwright não disponível"}
        page = self.new_page()
        if engine.lower() == "bing":
            url = f"https://www.bing.com/search?q={query}"
        else:
            url = f"https://www.google.com/search?q={query}"
        try:
            page.goto(url)
            content = page.content()
            return {"content": content}
        except PWTimeoutError as e:
            return {"error": f"timeout: {e}"}
        except Exception as e:
            return {"error": str(e)}

    def open_url(self, url: str):
        if not PLAYWRIGHT_AVAILABLE:
            return {"error": "Playwright não disponível"}
        page = self.new_page()
        try:
            page.goto(url)
            return {"content": page.content()}
        except Exception as e:
            return {"error": str(e)}

    def click(self, selector: str):
        if not PLAYWRIGHT_AVAILABLE or self.page is None:
            return {"error": "sem página ativa"}
        try:
            self.page.click(selector)
            return {"ok": True}
        except Exception as e:
            return {"error": str(e)}

    def type(self, selector: str, text: str, delay: int = 10):
        if not PLAYWRIGHT_AVAILABLE or self.page is None:
            return {"error": "sem página ativa"}
        try:
            self.page.fill(selector, "")
            self.page.type(selector, text, delay=delay)
            return {"ok": True}
        except Exception as e:
            return {"error": str(e)}

    def screenshot(self, path: str = "screenshot.png", selector: Optional[str] = None):
        if not PLAYWRIGHT_AVAILABLE or self.page is None:
            return {"error": "sem página ativa"}
        try:
            if selector:
                el = self.page.query_selector(selector)
                if el:
                    el.screenshot(path=path)
                else:
                    return {"error": "selector não encontrado"}
            else:
                self.page.screenshot(path=path, full_page=True)
            return {"ok": True, "path": path}
        except Exception as e:
            return {"error": str(e)}

    def eval_js(self, script: str):
        if not PLAYWRIGHT_AVAILABLE or self.page is None:
            return {"error": "sem página ativa"}
        try:
            return {"result": self.page.evaluate(script)}
        except Exception as e:
            return {"error": str(e)}

    def close(self):
        try:
            if self.page:
                self.page.close()
            if self.context:
                self.context.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
        except Exception:
            pass

class ONTokenizerWrapper:
    def __init__(self, vocab_size:int=DEFAULT_VOCAB_SIZE, path_out: str = "on_tokenizer.json"):
        self.vocab_size = vocab_size
        self.path_out = path_out
        self.tokenizer = Tokenizer(models.BPE())
        self.tokenizer.normalizer = normalizers.NFKC()
        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
        self.trainer = trainers.BpeTrainer(vocab_size=self.vocab_size, special_tokens=["[PAD]","[UNK]","[CLS]","[SEP]"])
        self._is_trained = False

    def train_from_iterator(self, iterator: Iterator[str]):
        print("[Tokenizer] Treinando BPE (pode demorar)...")
        self.tokenizer.train_from_iterator(iterator, trainer=self.trainer)
        self.tokenizer.save(self.path_out)
        self._is_trained = True
        print(f"[Tokenizer] Salvo em {self.path_out}")

    def load(self):
        if os.path.exists(self.path_out):
            self.tokenizer = Tokenizer.from_file(self.path_out)
            self._is_trained = True
            print("[Tokenizer] Carregado.")
        else:
            raise FileNotFoundError("Tokenizer não encontrado.")

    def encode_ids(self, text: str, max_length: int = DEFAULT_MAX_SEQ) -> List[int]:
        ids = self.tokenizer.encode(text).ids
        return ids[:max_length] if len(ids) > max_length else ids

    def pad_truncate(self, ids: List[int], max_length: int = DEFAULT_MAX_SEQ) -> List[int]:
        if len(ids) >= max_length:
            return ids[:max_length]
        pad_id = self.tokenizer.token_to_id("[PAD]") if "[PAD]" in self.tokenizer.get_vocab() else 0
        return ids + [pad_id] * (max_length - len(ids))

    def vocab_size(self):
        return len(self.tokenizer.get_vocab())

def text_stream_iterator(hf_id:str, config_name: Optional[str]=None, field_candidates: List[str]=["text","article","content"]):
    try:
        if config_name:
            ds = load_dataset(hf_id, config_name, split="train", streaming=True)
        else:
            ds = load_dataset(hf_id, split="train", streaming=True)
    except Exception as e:
        print(f"[Dataset] Não foi possível abrir {hf_id}: {e}")
        return iter([])
    def gen():
        for ex in ds:
            if isinstance(ex, dict):
                for k in field_candidates:
                    if k in ex and isinstance(ex[k], str):
                        yield ex[k]; break
            elif isinstance(ex, str):
                yield ex
    return gen()

def image_dataset_iterator(hf_id:str):
    try:
        ds = load_dataset(hf_id, split="train")
    except Exception as e:
        print(f"[Dataset] Imagem {hf_id} inacessível: {e}")
        return iter([])
    def gen():
        for ex in ds:
            img = None
            if isinstance(ex, dict):
                if "image" in ex: img = ex["image"]
                elif "img" in ex: img = ex["img"]
            if img is not None:
                caption = ex.get("caption","") if isinstance(ex, dict) else ""
                yield img, caption
    return gen()

def audio_dataset_iterator(hf_id:str, config_name:Optional[str]=None):
    try:
        if config_name:
            ds = load_dataset(hf_id, config_name, split="train", streaming=True)
        else:
            ds = load_dataset(hf_id, split="train", streaming=True)
    except Exception as e:
        print(f"[Dataset] Áudio {hf_id} inacessível: {e}")
        return iter([])
    def gen():
        for ex in ds:
            if isinstance(ex, dict) and "audio" in ex:
                yield ex["audio"], ex.get("sentence", "") or ex.get("text", "")
    return gen()

class VisionEncoder(nn.Module):
    def __init__(self, embed_dim:int):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),
            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1,1)),
            nn.Flatten()
        )
        self.fc = nn.Linear(64, embed_dim)

    def forward(self, img_tensor):
        feat = self.conv(img_tensor)
        return self.fc(feat)

class ChunkAudioEncoder(nn.Module):
    def __init__(self, embed_dim:int, sample_rate:int=16000, n_mels:int=80, chunk_seconds:float=30.0):
        super().__init__()
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.chunk_samples = int(chunk_seconds * sample_rate)
        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)
        self.chunk_cnn = nn.Sequential(
            nn.Conv1d(n_mels, 128, 3, 2, 1), nn.ReLU(),
            nn.AdaptiveAvgPool1d(1), nn.Flatten()
        )
        self.fc = nn.Linear(128, embed_dim)
        self.agg_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*2)

    def forward(self, wave: torch.Tensor):
        device = wave.device
        B, T = wave.shape
        chunk_embs = []
        for start in range(0, T, self.chunk_samples):
            seg = wave[:, start:start+self.chunk_samples]
            if seg.shape[1] == 0: break
            if seg.shape[1] < self.chunk_samples:
                pad = torch.zeros(B, self.chunk_samples - seg.shape[1], device=device)
                seg = torch.cat([seg, pad], dim=1)
            mel = self.melspec(seg)
            emb = self.chunk_cnn(mel)
            emb = self.fc(emb)
            chunk_embs.append(emb.unsqueeze(0))
        if len(chunk_embs) == 0:
            return torch.zeros(B, self.fc.out_features, device=device)
        seq = torch.cat(chunk_embs, dim=0)
        seq = self.agg_layer(seq)
        pooled = seq.mean(dim=0)
        return pooled

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim:int, num_heads:int, dropout:float=0.0):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.qkv = nn.Linear(embed_dim, 3*embed_dim, bias=False)
        self.out = nn.Linear(embed_dim, embed_dim)
        self.drop = nn.Dropout(dropout)

    def forward(self, x, mask:Optional[torch.Tensor]=None):
        seq, bsz, _ = x.size()
        qkv = self.qkv(x).view(seq, bsz, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.unbind(dim=2)
        q = q.permute(1,2,0,3); k = k.permute(1,2,0,3); v = v.permute(1,2,0,3)
        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.head_dim)
        if mask is not None: scores = scores.masked_fill(mask==0, float("-inf"))
        att = torch.softmax(scores, dim=-1)
        out = torch.matmul(att, v)
        out = out.permute(2,0,1,3).contiguous().view(seq, bsz, self.embed_dim)
        return self.drop(self.out(out))

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim:int, num_heads:int, ffn_dim:int, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(nn.Linear(embed_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embed_dim))
        self.drop = nn.Dropout(dropout)

    def forward(self, x, mask:Optional[torch.Tensor]=None):
        r = x
        x = self.norm1(x)
        x = self.attn(x, mask)
        x = r + self.drop(x)
        r = x
        x = self.norm2(x)
        x = self.ff(x)
        return r + self.drop(x)

class ThoughtModule(nn.Module):
    def __init__(self, embed_dim:int, thought_minutes:float=9.0, token_rate:float=1.0,
                 n_layers:int=2, n_heads:int=8, max_steps:int=4096):
        super().__init__()
        computed = max(8, int(thought_minutes * 60 * token_rate))
        self.n_steps = min(computed, max_steps)
        self.start_tokens = nn.Parameter(torch.randn(self.n_steps, 1, embed_dim))
        self.layers = nn.ModuleList([TransformerBlock(embed_dim, n_heads, embed_dim*2) for _ in range(n_layers)])
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.kv_proj = nn.Linear(embed_dim, embed_dim * 2)
        self.memory = nn.Parameter(torch.randn(max(16, self.n_steps//4), 1, embed_dim))
        self.norm = nn.LayerNorm(embed_dim)
        self.mem_gate = nn.Linear(embed_dim * 2, embed_dim)

    def cross_attention(self, queries, context):
        Q, B, E = queries.size()
        C = context.size(0)
        q = self.q_proj(queries)
        kv = self.kv_proj(context)
        k, v = kv.chunk(2, dim=-1)
        q2 = q.permute(1,0,2)
        k2 = k.permute(1,0,2)
        scores = torch.matmul(q2, k2.transpose(-2,-1)) / math.sqrt(E)
        attn = torch.softmax(scores, dim=-1)
        v2 = v.permute(1,0,2)
        out = torch.matmul(attn, v2)
        out = out.permute(1,0,2)
        return out

    def forward(self, context: torch.Tensor):
        seq_len, bsz, embed = context.size()
        thoughts = self.start_tokens.expand(-1, bsz, -1)
        mem = self.memory.expand(-1, bsz, -1)
        full_context = torch.cat([context, mem], dim=0)
        combined = torch.cat([full_context, thoughts], dim=0)
        for layer in self.layers:
            combined = layer(combined)
        thoughts_out = combined[-self.n_steps:, :, :]
        cross = self.cross_attention(thoughts_out, full_context)
        gated = torch.sigmoid(self.mem_gate(torch.cat([thoughts_out, cross], dim=-1))) * cross + thoughts_out
        return self.norm(gated)

class ONModel(nn.Module):
    def __init__(self, vocab_size:int, embed_dim:int=DEFAULT_EMBED, num_layers:int=DEFAULT_LAYERS, num_heads:int=DEFAULT_HEADS, max_seq:int=DEFAULT_MAX_SEQ,
                 thought_minutes:float=9.0, token_rate:float=1.0, thought_max_steps:int=4096, thought_layers:int=2):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.max_seq = max_seq
        self.token_emb = nn.Embedding(vocab_size, embed_dim)
        self.pos_emb = nn.Parameter(torch.randn(max_seq, 1, embed_dim))
        self.layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads, embed_dim*4) for _ in range(num_layers)])
        self.thought = ThoughtModule(embed_dim, thought_minutes=thought_minutes, token_rate=token_rate, n_layers=thought_layers, n_heads=max(8, num_heads//2), max_steps=thought_max_steps)
        self.final_ln = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, vocab_size)
        self.vision_adapter = nn.Linear(embed_dim, embed_dim)
        self.audio_adapter = nn.Linear(embed_dim, embed_dim)

    def forward(self, input_ids: torch.LongTensor, vision_emb: Optional[torch.Tensor]=None, audio_emb: Optional[torch.Tensor]=None):
        bsz, seq = input_ids.size()
        x = self.token_emb(input_ids).transpose(0,1)
        x = x + self.pos_emb[:seq,:,:].to(x.device)
        prefix = []
        if vision_emb is not None:
            v = self.vision_adapter(vision_emb).unsqueeze(0)
            prefix.append(v)
        if audio_emb is not None:
            a = self.audio_adapter(audio_emb).unsqueeze(0)
            prefix.append(a)
        if prefix:
            pref = torch.cat(prefix, dim=0)
            x = torch.cat([pref, x], dim=0)
        for layer in self.layers:
            x = layer(x)
        thought_repr = self.thought(x)
        x = torch.cat([x, thought_repr], dim=0)
        x = self.final_ln(x)
        token_slice = x[-seq:,:,:].transpose(0,1)
        logits = self.head(token_slice)
        return logits

class ConvEncoder(nn.Module):
    def __init__(self, in_ch=3, z_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, 64, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((4,4)),
            nn.Flatten(),
            nn.Linear(256*4*4, z_dim*2)
        )

    def forward(self, x):
        stats = self.net(x)
        mu, logvar = stats.chunk(2, dim=1)
        return mu, logvar

class ConvDecoder(nn.Module):
    def __init__(self, out_ch=3, z_dim=512):
        super().__init__()
        self.fc = nn.Linear(z_dim, 256*4*4)
        self.net = nn.Sequential(
            nn.Unflatten(1, (256,4,4)),
            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, out_ch, 4, 2, 1), nn.Sigmoid()
        )

    def forward(self, z):
        x = self.fc(z)
        x = self.net(x)
        return x

class VAE(nn.Module):
    def __init__(self, img_channels=3, z_dim=512):
        super().__init__()
        self.encoder = ConvEncoder(in_ch=img_channels, z_dim=z_dim)
        self.decoder = ConvDecoder(out_ch=img_channels, z_dim=z_dim)

    def reparameterize(self, mu, logvar):
        std = (0.5 * logvar).exp()
        eps = torch.randn_like(std)
        return mu + eps * std

    def encode(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        z, mu, logvar = self.encode(x)
        recon = self.decode(z)
        return recon, mu, logvar

class SimpleUNet(nn.Module):
    def __init__(self, latent_dim=512):
        super().__init__()
        self.down = nn.Sequential(
            nn.Linear(latent_dim, latent_dim*2), nn.GELU(),
            nn.Linear(latent_dim*2, latent_dim)
        )
        self.time_embed = nn.Sequential(nn.Linear(1, latent_dim), nn.ReLU())
        self.out = nn.Sequential(nn.Linear(latent_dim, latent_dim), nn.Tanh())

    def forward(self, z, t):
        te = self.time_embed(t.unsqueeze(-1).float())
        h = self.down(z)
        h = h + te
        return self.out(h)

def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):
    return torch.linspace(beta_start, beta_end, timesteps)

class Diffusion:
    def __init__(self, model: nn.Module, timesteps: int = 1000, device=DEVICE):
        self.model = model
        self.timesteps = timesteps
        self.device = device
        betas = linear_beta_schedule(timesteps).to(device)
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        self.betas = betas
        self.alphas = alphas
        self.alphas_cumprod = alphas_cumprod
        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)

    def q_sample(self, x_start, t, noise=None):
        if noise is None:
            noise = torch.randn_like(x_start)
        sqrt_acp = self.sqrt_alphas_cumprod[t].unsqueeze(-1)
        sqrt_om = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(-1)
        return sqrt_acp * x_start + sqrt_om * noise

    def p_losses(self, x_start, t):
        noise = torch.randn_like(x_start)
        x_noisy = self.q_sample(x_start, t, noise=noise)
        t_tensor = t.float() / float(self.timesteps)
        pred = self.model(x_noisy, t_tensor)
        return F.mse_loss(pred, noise)

    @torch.no_grad()
    def sample(self, shape, device, steps=None):
        steps = steps or self.timesteps
        x = torch.randn(shape, device=device)
        for i in reversed(range(steps)):
            t = torch.tensor([i], device=device)
            t_norm = t.float() / float(self.timesteps)
            pred_noise = self.model(x, t_norm)
            beta = self.betas[i]
            alpha = self.alphas[i]
            alpha_cum = self.alphas_cumprod[i]
            if i > 0:
                noise = torch.randn_like(x)
            else:
                noise = torch.zeros_like(x)
            x = (1 / math.sqrt(alpha)) * (x - (beta / math.sqrt(1 - alpha_cum)) * pred_noise) + math.sqrt(beta) * noise
        return x

class TiledGenerator:
    def __init__(self, vae: VAE, unet: SimpleUNet, tile_size:int=1024, overlap:int=64, device=DEVICE):
        self.vae = vae
        self.unet = unet
        self.tile_size = tile_size
        self.overlap = overlap
        self.device = device

    def tile_coords(self, W:int, H:int):
        stride = self.tile_size - self.overlap
        xs = list(range(0, max(1, W - self.overlap), stride))
        ys = list(range(0, max(1, H - self.overlap), stride))
        for y in ys:
            for x in xs:
                w = min(self.tile_size, W - x)
                h = min(self.tile_size, H - y)
                yield x, y, w, h

    def generate_tile(self, prompt, W, H, x, y, w, h, diffusion: Diffusion, steps=200):
        latent_shape = (1, 512)
        z = diffusion.sample(latent_shape, device=self.device, steps=steps)
        recon = self.vae.decode(z)
        recon_img = F.interpolate(recon, size=(h,w), mode="bilinear", align_corners=False)
        arr = (recon_img.clamp(0,1).cpu().squeeze(0).permute(1,2,0).numpy() * 255).astype('uint8')
        return Image.fromarray(arr)

    def generate_gigapixel(self, prompt, W, H, diffusion: Diffusion, steps=200):
        canvas = Image.new("RGB", (W, H))
        for x,y,w,h in self.tile_coords(W,H):
            tile = self.generate_tile(prompt, W, H, x, y, w, h, diffusion, steps=steps)
            canvas.paste(tile, (x,y))
        return canvas

image_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])

def collate_texts(tokenizer: ONTokenizerWrapper, texts: List[str], max_len:int=DEFAULT_MAX_SEQ):
    ids = [tokenizer.encode_ids(t, max_length=max_len) for t in texts]
    ids = [tokenizer.pad_truncate(i, max_length=max_len) for i in ids]
    return torch.tensor(ids, dtype=torch.long)

def train_real(args):
    tk = ONTokenizerWrapper(vocab_size=args.vocab_size, path_out=args.tokenizer_path)
    if os.path.exists(args.tokenizer_path):
        tk.load()
    else:
        def combined_iter():
            for ds in args.text_datasets:
                for s in text_stream_iterator(ds):
                    yield s
        tk.train_from_iterator(combined_iter())

    text_iters = {ds: text_stream_iterator(ds) for ds in args.text_datasets}
    image_iters = {ds: image_dataset_iterator(ds) for ds in args.image_datasets}
    audio_iters = {ds: audio_dataset_iterator(ds) for ds in args.audio_datasets}

    vocab_size = len(tk.tokenizer.get_vocab())
    model = ONModel(
        vocab_size=vocab_size,
        embed_dim=args.embed_dim,
        num_layers=args.layers,
        num_heads=args.heads,
        max_seq=args.max_seq,
        thought_minutes=args.thought_minutes,
        token_rate=args.token_rate,
        thought_max_steps=args.max_thought_steps,
        thought_layers=args.thought_layers
    ).to(args.device)

    vision_enc = VisionEncoder(args.embed_dim).to(args.device)
    audio_enc = ChunkAudioEncoder(args.embed_dim, chunk_seconds=args.audio_chunk_seconds).to(args.device)

    vae = VAE(img_channels=3, z_dim=args.vae_z_dim).to(args.device)
    unet = SimpleUNet(latent_dim=args.vae_z_dim).to(args.device)
    diffusion = Diffusion(unet, timesteps=args.diffusion_steps, device=args.device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    optimizer_vae = torch.optim.AdamW(vae.parameters(), lr=args.lr)
    optimizer_unet = torch.optim.AdamW(unet.parameters(), lr=args.lr)

    nav = None
    if args.enable_browser:
        if PLAYWRIGHT_AVAILABLE:
            nav = MiniNavigator(headless=not args.browser_headful, timeout=args.browser_timeout)
            print("[TRAIN] MiniNavigator iniciado.")
        else:
            print("[TRAIN] enable_browser=True, mas Playwright não está instalado. Ignorando navegador.")

    steps = 0
    print("[TRAIN] Iniciando loop de treino (esqueleto + VAE/diffusion + navegador opcional).")
    for epoch in range(args.epochs):
        print(f"[TRAIN] Época {epoch+1}/{args.epochs}")
        for ds_name, iterator in text_iters.items():
            print(f"  Dataset text: {ds_name}")
            batch_texts = []
            for i, txt in enumerate(iterator):
                if not isinstance(txt, str): continue
                batch_texts.append(txt)
                if len(batch_texts) == args.batch_size:
                    input_ids = collate_texts(tk, batch_texts, max_len=args.max_seq).to(args.device)

                    vision_emb = None
                    audio_emb = None

                    for img_ds, img_it in image_iters.items():
                        try:
                            img, caption = next(img_it)
                            if isinstance(img, dict) and "path" in img:
                                img = Image.open(img["path"]).convert("RGB")
                            if isinstance(img, Image.Image):
                                t = image_transform(img).unsqueeze(0).to(args.device)
                                vision_emb = vision_enc(t)
                            break
                        except StopIteration:
                            continue
                        except Exception:
                            continue

                    for a_ds, a_it in audio_iters.items():
                        try:
                            aud, meta = next(a_it)
                            if isinstance(aud, dict) and "array" in aud:
                                arr = torch.tensor(aud["array"]).float().unsqueeze(0).to(args.device)
                                audio_emb = audio_enc(arr)
                            break
                        except StopIteration:
                            continue
                        except Exception:
                            continue

                    web_text = None
                    if nav is not None and args.browser_augment and batch_texts:
                        q = batch_texts[0][:200]
                        r = nav.search(q)
                        if isinstance(r, dict) and "content" in r:
                            web_text = r["content"][: args.browser_max_content_chars]
                            batch_texts[0] = batch_texts[0] + "\n\n[WEB_AUG] " + web_text
                            input_ids = collate_texts(tk, batch_texts, max_len=args.max_seq).to(args.device)

                    logits = model(input_ids, vision_emb=vision_emb, audio_emb=audio_emb)
                    labels = input_ids
                    ignore_idx = tk.tokenizer.token_to_id("[PAD]") if "[PAD]" in tk.tokenizer.get_vocab() else -100
                    loss_f = nn.CrossEntropyLoss(ignore_index=ignore_idx)
                    loss = loss_f(logits.view(-1, logits.size(-1)), labels.view(-1))
                    optimizer.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()

                    if vision_emb is not None and args.train_vae_diffusion:
                        img_tensor = image_transform(img).unsqueeze(0).to(args.device)
                        recon, mu, logvar = vae(img_tensor)
                        rec_loss = F.mse_loss(recon, img_tensor) + 1e-6 * torch.mean(torch.exp(logvar) + mu**2 - logvar)
                        optimizer_vae.zero_grad(); rec_loss.backward(); optimizer_vae.step()

                        with torch.no_grad():
                            z, mu_z, logvar_z = vae.encode(img_tensor)
                        t_idx = torch.randint(0, diffusion.timesteps, (z.shape[0],), device=args.device)
                        diff_loss = diffusion.p_losses(z, t_idx)
                        optimizer_unet.zero_grad(); diff_loss.backward(); optimizer_unet.step()

                    steps += 1
                    if steps % args.log_every == 0:
                        print(f"    Step {steps} loss: {loss.item():.4f}")

                    batch_texts = []
                if i > args.max_examples_per_dataset:
                    break

        ckpt = os.path.join(args.checkpoint_dir, f"on_epoch{epoch+1}.pt")
        os.makedirs(args.checkpoint_dir, exist_ok=True)
        torch.save({
            "model": model.state_dict(),
            "optim": optimizer.state_dict(),
            "vae": vae.state_dict(),
            "opt_vae": optimizer_vae.state_dict(),
            "unet": unet.state_dict(),
            "opt_unet": optimizer_unet.state_dict(),
        }, ckpt)
        print(f"[TRAIN] Checkpoint salvo: {ckpt}")

    if nav is not None:
        nav.close()
    print("[TRAIN] Loop finalizado.")

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--vocab_size", type=int, default=50000)
    p.add_argument("--max_seq", type=int, default=DEFAULT_MAX_SEQ)
    p.add_argument("--embed_dim", type=int, default=DEFAULT_EMBED)
    p.add_argument("--layers", type=int, default=DEFAULT_LAYERS)
    p.add_argument("--heads", type=int, default=DEFAULT_HEADS)
    p.add_argument("--batch_size", type=int, default=DEFAULT_BATCH)
    p.add_argument("--epochs", type=int, default=1)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--device", type=str, default=str(DEVICE))
    p.add_argument("--tokenizer_path", type=str, default="on_tokenizer.json")
    p.add_argument("--checkpoint_dir", type=str, default=CHECKPOINT_DIR)
    p.add_argument("--log_every", type=int, default=10)
    p.add_argument("--max_examples_per_dataset", type=int, default=200)
    p.add_argument("--text_datasets", nargs="+", default=DATASET_IDS_TEXT)
    p.add_argument("--image_datasets", nargs="+", default=DATASET_IDS_IMAGE)
    p.add_argument("--audio_datasets", nargs="+", default=DATASET_IDS_AUDIO)
    p.add_argument("--audio_chunk_seconds", type=float, default=30.0, help="tamanho (s) do chunk de áudio para encoder longo")
    p.add_argument("--vae_z_dim", type=int, default=512, help="dimensão latente do VAE")
    p.add_argument("--diffusion_steps", type=int, default=200, help="timesteps do DDPM (treino/sampling)")
    p.add_argument("--train_vae_diffusion", action="store_true", help="se setado, treinar VAE+diffusion durante loop")
    p.add_argument("--thought_minutes", type=float, default=9.0, help="tempo (min) de 'thinking' para ThoughtModule (heurística)")
    p.add_argument("--token_rate", type=float, default=1.0, help="taxa heurística (tokens por segundo) para converter minutos em passos")
    p.add_argument("--max_thought_steps", type=int, default=4096, help="limite máximo absoluto de thought-steps (proteção de memória)")
    p.add_argument("--thought_layers", type=int, default=2, help="número de layers internas do ThoughtModule")
    p.add_argument("--enable_browser", action="store_true", help="habilita mini-navegador (Playwright) durante treino para augment/inspeção")
    p.add_argument("--browser_headful", action="store_true", help="abre navegador não-headless (útil para debug)")
    p.add_argument("--browser_timeout", type=int, default=15000, help="timeout (ms) do Playwright")
    p.add_argument("--browser_augment", action="store_true", help="se setado, usa conteúdo web como augment para batches")
    p.add_argument("--browser_max_content_chars", type=int, default=2000, help="quantos chars de conteúdo web incluir por batch (se augment ativado)")
    p.add_argument("--tokenizer_vocab_estimate", type=int, default=50000, help="apenas estimativa de vocab para teste")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    args.device = torch.device(args.device)
    train_real(args)
